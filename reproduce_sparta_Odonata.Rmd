---
title: "preliminary_analysis_Odonata"
author: "Diana Bowler"
date: "24 januar 2018"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

```

#Analysis of the Odonata data from Saarland

First step: read in the raw data file from the sMon data portal.
Sheet 5 of the workbook contains the raw data of occurrences.
Each row in the data frame is a species observation.

```{r warning=FALSE}

library(gdata)
df <- read.xls("raw-data/Odonata_Saarland_Trockur.xls",sheet=5)

```

#Data frame formatting

Step one: format the date and extract month, dat and year information.

```{r}

library(lubridate)
df$Date <- as.Date(df$Date,file="%Y-%m-%d")
df$month <- month(df$Date)
df$day <- yday(df$Date)
df$year <- year(df$Date)

```

Step two: remove those with missing date entries

```{r}

df <- subset(df,!is.na(month))

```


#Data filtering

Filter one: focus on the months that were most sampled. The results suggest this is between April and September.

```{r}

table(df$month)

df <- subset(df, month %in% c(4:9))

```

Filter two: begin the time series from the first year with reasonable amount of data (i.e., number of records). 1981 seems like a good place to start. Also, we cut off 2017 incase the database is not up-to-date yet.

```{r}

table(df$Year)
df <- subset(df, Year>1980)
df <- subset(df, Year<2017)

```

#Data availability/Sampling effort checking

We can look at effort by: total number of records and average number of species seen per sampling day ("list length")

```{r}

library(sparta)

dataDiagnostics(taxa= as.character(df$Species), 
                site= as.character(df$MTB_Q), 
                time_period = df$Date)

```

Organise data using the sparta function

```{r}

tempDF <- formatOccData(taxa= as.character(df$Species), 
                site= as.character(df$MTB_Q), 
                time_period = df$Date,
                includeJDay = TRUE)

#get occurence matrix
occurrenceDF <- tempDF$spp_vis

#put out list length for each visit
listlengthDF <- tempDF$occDetdata

```

Compile data for bugs (for a select species  - here )

```{r}

bugs.data <- list(nsite = length(unique(listlengthDF$site)),
                  nyear = length(unique(listlengthDF$year)),
                  nvisit = nrow(listlengthDF),
                  site = as.numeric(factor(listlengthDF$site)),
                  year = as.numeric(factor(listlengthDF$year)),
                  Jul_date = listlengthDF$Jul_date - median(listlengthDF$Jul_date),
                  L = listlengthDF$L - median(listlengthDF$L),
                  y = ifelse(occurrenceDF[,"Aeshna affinis"] == FALSE,0,1))
                  
```

Run model

```{r}
source('R/BUGS_misc_functions.R')

params <- c("psi.fs","beta1","beta1","dtype.p","mu.lp")

out1 <- jags(bugs.data, inits=NULL, params, "BUGS_sparta.txt", n.thin=nt,
               n.chains=3, n.burnin=500,n.iter=2000,parallel = T)
print(out1,2)

```


Model checking

```{r}

#http://xavier-fim.net/packages/ggmcmc/
library(ggmcmc)
bayes.mod.fit.gg <- ggs(out1$samples,family="psi.fs")
ggs_density(bayes.mod.fit.gg)
ggs_histogram(bayes.mod.fit.gg)
ggs_traceplot(bayes.mod.fit.gg)
ggs_running(bayes.mod.fit.gg)
ggs_compare_partial(bayes.mod.fit.gg)
ggs_autocorrelation(bayes.mod.fit.gg)
ggmcmc(bayes.mod.fit.gg, file = "bayes_fit_ggmcmc.pdf")

```

Add predictions to the data (to be finished)

```{r}

out2<-update(out1,parameters.to.save="expNuIndivs.new",n.iter=1000)
out3<-ggs(out2$samples,family="expNuIndivs.new")
my_ggs_density(out3)

my_ggs_ppmean(D=out3,outcome=as.numeric(my.n))

```



